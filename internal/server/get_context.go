package server

import (
	"context"
	"fmt"
	"sort"
	"strings"
	"time"

	"connectrpc.com/connect"
	"github.com/hsn0918/rag/internal/adapters"
	ragv1 "github.com/hsn0918/rag/internal/gen/rag/v1"
	"github.com/hsn0918/rag/pkg/clients/openai"
	pkgopenai "github.com/hsn0918/rag/pkg/clients/openai"
	"github.com/hsn0918/rag/pkg/logger"
	"github.com/hsn0918/rag/pkg/prompts"
	"github.com/hsn0918/rag/pkg/search"
	pkgutils "github.com/hsn0918/rag/pkg/utils"
	"log/slog"
)

type contextStages struct {
	query         string
	keywords      []string
	queryText     string
	queryVector   []float32
	similarChunks []adapters.ChunkSearchResult
	rankedChunks  []adapters.ChunkSearchResult
}

// GetContext implements intelligent document retrieval and question-answering
// functionality using RAG (Retrieval-Augmented Generation).
//
// The complete RAG pipeline consists of:
//  1. Extract keywords from user query using LLM
//  2. Generate query embeddings for semantic search
//  3. Perform intelligent reranking of search results
//  4. Generate personalized responses using LLM
//
// This method handles the entire request lifecycle including logging,
// error handling, and fallback strategies when services are unavailable.
func (s *RagServer) GetContext(
	ctx context.Context,
	req *connect.Request[ragv1.GetContextRequest],
) (*connect.Response[ragv1.GetContextResponse], error) {
	startTime := time.Now()
	query := req.Msg.GetQuery()

	if query == "" {
		return nil, connect.NewError(connect.CodeInvalidArgument, fmt.Errorf("query is required"))
	}

	logger.Get().Info("å¼€å§‹å¤„ç†æ™ºèƒ½æ–‡æ¡£æ£€ç´¢è¯·æ±‚",
		slog.String("query", query),
		slog.Int("query_length", len(query)),
		slog.String("request_id", req.Header().Get("X-Request-ID")),
		slog.Time("start_time", startTime),
	)

	stage := &contextStages{query: query}

	if err := s.runKeywordStage(ctx, stage); err != nil {
		return nil, err
	}
	if err := s.runEmbeddingStage(ctx, stage); err != nil {
		return nil, err
	}
	if err := s.runSearchStage(ctx, stage); err != nil {
		return nil, err
	}
	if len(stage.similarChunks) == 0 {
		logger.Get().Warn("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
			slog.String("query", stage.query),
			slog.Any("keywords", stage.keywords),
			slog.Duration("total_duration", time.Since(startTime)),
		)
		return connect.NewResponse(&ragv1.GetContextResponse{
			Context: fmt.Sprintf("æœªæ‰¾åˆ°ä¸æŸ¥è¯¢ '%s' ç›¸å…³çš„å†…å®¹ã€‚è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯ã€‚", stage.query),
		}), nil
	}

	// ç¬¬å››æ­¥ï¼šæ™ºèƒ½é‡æ’åº - ç»¼åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…
	logger.Get().Debug("å¼€å§‹æ™ºèƒ½é‡æ’åº",
		slog.Int("chunks_before", len(stage.similarChunks)),
	)
	rerankStart := time.Now()
	stage.rankedChunks = s.rerankChunksWithKeywords(stage.similarChunks, stage.query, stage.keywords)
	rerankDuration := time.Since(rerankStart)

	logger.Get().Info("é‡æ’åºå®Œæˆ",
		slog.Int("chunks_after", len(stage.rankedChunks)),
		slog.Duration("rerank_duration", rerankDuration),
	)

	if logger.Get().Enabled(ctx, slog.LevelDebug) && len(stage.rankedChunks) > 0 {
		for i, chunk := range stage.rankedChunks {
			if i < 3 { // Log top 3 reranked results
				logger.Get().Debug("é‡æ’åºç»“æœ",
					slog.Int("final_rank", i+1),
					slog.String("chunk_id", chunk.ChunkID),
					slog.Float64("similarity", float64(chunk.Similarity)),
					slog.Any("advanced_score", chunk.Metadata["advanced_score"]),
				)
			}
		}
	}
	// ç¬¬äº”æ­¥ï¼šä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–æ€»ç»“å›ç­”
	logger.Get().Debug("å¼€å§‹ç”Ÿæˆä¸ªæ€§åŒ–å›ç­”",
		slog.String("query", stage.query),
		slog.Int("chunks_count", len(stage.rankedChunks)),
	)
	summaryStart := time.Now()
	contextContent, err := s.generateContextSummary(ctx, stage.rankedChunks, stage.query)
	summaryDuration := time.Since(summaryStart)

	if err != nil {
		logger.Get().Error("å¤§æ¨¡å‹æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°æ¨¡æ¿å›ç­”",
			slog.Any("error", err),
			slog.Duration("failed_duration", summaryDuration),
		)
		// é™çº§åˆ°æ¨¡æ¿å›ç­”
		contextContent = s.buildContextResponse(stage.rankedChunks, stage.query)
	} else {
		logger.Get().Info("ä¸ªæ€§åŒ–å›ç­”ç”ŸæˆæˆåŠŸ",
			slog.Duration("summary_duration", summaryDuration),
			slog.Int("summary_length", len(contextContent)),
		)
	}

	// è®¡ç®—å¤„ç†æ—¶é—´
	processingTime := time.Since(startTime).Milliseconds()

	totalDuration := time.Since(startTime)
	logger.Get().Info("æ™ºèƒ½æ–‡æ¡£æ£€ç´¢å®Œæˆ",
		slog.String("query", stage.query),
		slog.Int("chunks_found", len(stage.similarChunks)),
		slog.Int("chunks_used", len(stage.rankedChunks)),
		slog.Int("response_length", len(contextContent)),
		slog.Int64("processing_time_ms", processingTime),
		slog.Duration("total_duration", totalDuration),
	)

	return connect.NewResponse(&ragv1.GetContextResponse{
		Context: contextContent,
	}), nil
}

func (s *RagServer) runKeywordStage(ctx context.Context, stage *contextStages) error {
	logger.Get().Debug("å¼€å§‹æå–å…³é”®è¯", slog.String("query", stage.query))
	start := time.Now()
	keywords, err := s.generateKeywords(ctx, stage.query)
	duration := time.Since(start)

	if err != nil {
		logger.Get().Error("å¤§æ¨¡å‹å…³é”®è¯æå–å¤±è´¥",
			slog.Any("error", err),
			slog.Duration("duration", duration),
		)
		return connect.NewError(connect.CodeInternal, fmt.Errorf("failed to generate keywords: %w", err))
	}
	logger.Get().Info("å¤§æ¨¡å‹å…³é”®è¯æå–å®Œæˆ",
		slog.Any("keywords", keywords),
		slog.Int("keywords_count", len(keywords)),
		slog.Duration("duration", duration),
	)
	stage.keywords = keywords
	return nil
}

func (s *RagServer) runEmbeddingStage(ctx context.Context, stage *contextStages) error {
	stage.queryText = strings.Join(stage.keywords, " ")
	if stage.queryText == "" {
		logger.Get().Debug("å…³é”®è¯ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ç”Ÿæˆå‘é‡")
		stage.queryText = stage.query
	}

	logger.Get().Debug("å¼€å§‹ç”ŸæˆæŸ¥è¯¢å‘é‡", slog.String("query_text", stage.queryText))
	start := time.Now()
	vec, err := s.generateEmbedding(ctx, stage.queryText)
	duration := time.Since(start)

	if err != nil {
		logger.Get().Error("æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥",
			slog.Any("error", err),
			slog.Duration("duration", duration),
		)
		return connect.NewError(connect.CodeInternal, fmt.Errorf("failed to generate query embedding: %w", err))
	}
	logger.Get().Debug("æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆ",
		slog.Int("vector_dim", len(vec)),
		slog.Duration("duration", duration),
	)
	stage.queryVector = vec
	return nil
}

func (s *RagServer) runSearchStage(ctx context.Context, stage *contextStages) error {
	start := time.Now()
	var (
		results []adapters.ChunkSearchResult
		err     error
	)

	if s.SearchOptimizer != nil {
		logger.Get().Debug("ä½¿ç”¨ä¼˜åŒ–æœç´¢ç­–ç•¥")
		results, err = s.SearchOptimizer.OptimizedSearch(ctx, stage.query, stage.queryVector)
		if err != nil {
			logger.Get().Error("ä¼˜åŒ–æœç´¢å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æœç´¢",
				slog.Any("error", err),
				slog.Duration("failed_duration", time.Since(start)),
			)
			results, err = s.searchSimilarChunks(ctx, stage.queryVector, 15)
			if err != nil {
				logger.Get().Error("å‘é‡ç›¸ä¼¼æ€§æœç´¢å¤±è´¥",
					slog.Any("error", err),
					slog.Duration("total_search_duration", time.Since(start)),
				)
				return connect.NewError(connect.CodeInternal, fmt.Errorf("failed to search similar chunks: %w", err))
			}
		}
	} else {
		logger.Get().Debug("ä½¿ç”¨æ ‡å‡†å‘é‡æœç´¢")
		results, err = s.searchSimilarChunks(ctx, stage.queryVector, 15)
		if err != nil {
			logger.Get().Error("å‘é‡ç›¸ä¼¼æ€§æœç´¢å¤±è´¥",
				slog.Any("error", err),
				slog.Duration("search_duration", time.Since(start)),
			)
			return connect.NewError(connect.CodeInternal, fmt.Errorf("failed to search similar chunks: %w", err))
		}
	}

	searchDuration := time.Since(start)
	logger.Get().Info("æœç´¢å®Œæˆ",
		slog.Int("chunks_found", len(results)),
		slog.Duration("search_duration", searchDuration),
	)

	if logger.Get().Enabled(ctx, slog.LevelDebug) {
		for i, chunk := range results {
			if i < 5 { // Only log top 5 for brevity
				logger.Get().Debug("æœç´¢ç»“æœè¯¦æƒ…",
					slog.Int("rank", i+1),
					slog.String("chunk_id", chunk.ChunkID),
					slog.Float64("similarity", float64(chunk.Similarity)),
					slog.Int("content_length", len(chunk.Content)),
				)
			}
		}
	}

	stage.similarChunks = results
	return nil
}

// searchSimilarChunks performs semantic vector search using pgvector.
//
// This function searches for similar document chunks in PostgreSQL based on
// the query vector, using cosine similarity to calculate relevance.
// It returns the most relevant document fragments up to the specified limit.
//
// The similarity threshold of 0.3 filters out low-relevance results.
func (s *RagServer) searchSimilarChunks(ctx context.Context, queryVector []float32, limit int) ([]adapters.ChunkSearchResult, error) {
	// ä½¿ç”¨æ•°æ®åº“çš„å‘é‡æœç´¢åŠŸèƒ½
	results, err := s.DB.SearchSimilarChunks(ctx, queryVector, limit, 0.3) // 0.3æ˜¯ç›¸ä¼¼åº¦é˜ˆå€¼
	if err != nil {
		return nil, fmt.Errorf("database search failed: %w", err)
	}

	logger.Get().Debug("Vector search completed",
		slog.Int("results_count", len(results)),
		slog.Int("query_vector_dim", len(queryVector)),
	)

	return results, nil
}

// rerankChunks reranks and filters search results based on multiple factors.
//
// This function is deprecated in favor of rerankChunksWithKeywords which
// provides more sophisticated ranking algorithms.
func (s *RagServer) rerankChunks(chunks []adapters.ChunkSearchResult, query string) []adapters.ChunkSearchResult {
	// åŸºäºå¤šä¸ªå› ç´ è¿›è¡Œé‡æ’åº
	sort.Slice(chunks, func(i, j int) bool {
		// ç»¼åˆè¯„åˆ†ï¼šç›¸ä¼¼åº¦ + å†…å®¹é•¿åº¦ + å…³é”®è¯åŒ¹é…
		scoreI := s.calculateChunkScore(chunks[i], query)
		scoreJ := s.calculateChunkScore(chunks[j], query)
		return scoreI > scoreJ
	})

	// è¿‡æ»¤ä½è´¨é‡ç»“æœï¼Œæœ€å¤šè¿”å›5ä¸ªæœ€ç›¸å…³çš„å—
	maxChunks := 5
	if len(chunks) > maxChunks {
		chunks = chunks[:maxChunks]
	}

	// è¿›ä¸€æ­¥è¿‡æ»¤ï¼šç§»é™¤ç›¸ä¼¼åº¦è¿‡ä½çš„ç»“æœ
	var filteredChunks []adapters.ChunkSearchResult
	for _, chunk := range chunks {
		if chunk.Similarity > 0.4 { // ç›¸ä¼¼åº¦é˜ˆå€¼
			filteredChunks = append(filteredChunks, chunk)
		}
	}

	logger.Get().Debug("Chunks reranked and filtered",
		slog.Int("original_count", len(chunks)),
		slog.Int("filtered_count", len(filteredChunks)),
	)

	return filteredChunks
}

// calculateChunkScore calculates a comprehensive score for a chunk.
//
// The scoring combines:
//   - Base similarity score (70% weight)
//   - Content length bonus for optimal lengths (10% weight)
//   - Keyword matching score (20% weight)
//
// This function is deprecated in favor of search.CalculateAdvancedScore.
func (s *RagServer) calculateChunkScore(chunk adapters.ChunkSearchResult, query string) float64 {
	// åŸºç¡€ç›¸ä¼¼åº¦æƒé‡
	score := float64(chunk.Similarity) * 0.7

	// å†…å®¹é•¿åº¦åŠ åˆ†ï¼ˆé€‚ä¸­é•¿åº¦æ›´å¥½ï¼‰
	contentLength := len(chunk.Content)
	if contentLength > 100 && contentLength < 1000 {
		score += 0.1
	}

	// å…³é”®è¯åŒ¹é…åŠ åˆ†
	queryLower := strings.ToLower(query)
	contentLower := strings.ToLower(chunk.Content)

	// ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
	queryWords := strings.Fields(queryLower)
	matchCount := 0
	for _, word := range queryWords {
		if strings.Contains(contentLower, word) {
			matchCount++
		}
	}

	if len(queryWords) > 0 {
		keywordScore := float64(matchCount) / float64(len(queryWords))
		score += keywordScore * 0.2
	}

	return score
}

// buildContextResponse constructs a structured context response from chunks.
//
// This function formats the search results into a human-readable response
// with proper structure, including similarity scores and metadata.
// It serves as a fallback when LLM-based summarization is unavailable.
func (s *RagServer) buildContextResponse(chunks []adapters.ChunkSearchResult, query string) string {
	if len(chunks) == 0 {
		return fmt.Sprintf("æœªæ‰¾åˆ°ä¸æŸ¥è¯¢ '%s' ç›¸å…³çš„å†…å®¹ã€‚", query)
	}

	var contextBuilder strings.Builder

	// æ·»åŠ æŸ¥è¯¢æ€»ç»“
	contextBuilder.WriteString(fmt.Sprintf("åŸºäºæŸ¥è¯¢ã€Œ%sã€æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³å†…å®¹ï¼š\n\n", query))

	// æ·»åŠ ç›¸å…³å†…å®¹å—
	for i, chunk := range chunks {
		contextBuilder.WriteString(fmt.Sprintf("## ç›¸å…³å†…å®¹ %d (ç›¸ä¼¼åº¦: %.2f)\n", i+1, chunk.Similarity))

		// æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹
		cleanContent := s.cleanAndFormatChunkContent(chunk.Content)
		contextBuilder.WriteString(cleanContent)
		contextBuilder.WriteString("\n\n")

		// æ·»åŠ å…ƒæ•°æ®ä¿¡æ¯
		if chunk.Metadata != nil {
			if chunkType, ok := chunk.Metadata["chunk_type"].(string); ok && chunkType != "" {
				contextBuilder.WriteString(fmt.Sprintf("*[å†…å®¹ç±»å‹: %s]*\n\n", chunkType))
			}
		}
	}

	// æ·»åŠ ä½¿ç”¨è¯´æ˜
	contextBuilder.WriteString("---\n")
	contextBuilder.WriteString("ğŸ’¡ æç¤ºï¼šä»¥ä¸Šå†…å®¹æŒ‰ç›¸å…³æ€§æ’åºï¼Œæ‚¨å¯ä»¥åŸºäºè¿™äº›ä¿¡æ¯è¿›è¡Œè¿›ä¸€æ­¥çš„è¯¢é—®ã€‚")

	return contextBuilder.String()
}

// cleanAndFormatChunkContent cleans and formats chunk content for display.
//
// This is a wrapper around utils.CleanAndFormatContent with a fixed
// maximum length of 2000 bytes.
func (s *RagServer) cleanAndFormatChunkContent(content string) string {
	return pkgutils.CleanAndFormatContent(content, 2000)
}

// generateKeywords extracts keywords from the query using LLM.
//
// This function calls the configured LLM service (e.g., DeepSeek) to perform
// intelligent keyword extraction with Chinese word segmentation.
// It automatically filters stop words and preserves technical terms and entities.
//
// If the LLM call fails, it falls back to basic local tokenization using
// utils.ExtractBasicKeywords.
//
// The function expects XML-formatted output from the LLM for structured parsing.
func (s *RagServer) generateKeywords(_ context.Context, query string) ([]string, error) {
	// Use prompt manager to get the keyword extraction prompt
	if s.promptEmbeddingService != nil {
		prompt, _, err := s.promptEmbeddingService.GetPromptWithEmbedding(prompts.PromptTypeKeywordExtraction)
		if err == nil {
			// Render the user prompt with the query
			userContent, err := s.promptEmbeddingService.GetPromptManager().RenderUserPrompt(
				prompts.PromptTypeKeywordExtraction,
				map[string]string{"query": query},
			)
			if err == nil {
				messages := []pkgopenai.Message{
					{
						Role:    "system",
						Content: prompt.System,
					},
					{
						Role:    "user",
						Content: userContent,
					},
				}

				resp, err := s.LLM.CreateChatCompletionWithDefaults(s.Config.Services.LLM.Model, messages)
				if err != nil {
					logger.Get().Error("LLMå…³é”®è¯æå–å¤±è´¥", slog.Any("error", err))
					return pkgutils.ExtractBasicKeywords(query), nil
				}

				if len(resp.Choices) == 0 {
					return pkgutils.ExtractBasicKeywords(query), nil
				}

				logger.Get().Info("å…³é”®è¯ LLM", slog.Any("resp", resp))
				content := resp.Choices[0].Message.Content
				keywords := s.parseKeywordsXML(content)

				if len(keywords) == 0 {
					// Fallback to line-based parsing if XML parsing fails
					lines := strings.Split(strings.TrimSpace(content), "\n")
					for _, line := range lines {
						keyword := strings.TrimSpace(line)
						if strings.HasPrefix(keyword, "<") && strings.HasSuffix(keyword, ">") {
							continue
						}
						if keyword != "" && len(keyword) > 1 {
							keywords = append(keywords, keyword)
						}
					}
				}

				if len(keywords) == 0 {
					return pkgutils.ExtractBasicKeywords(query), nil
				}

				return keywords, nil
			}
		}
	}

	// Fallback to direct prompt manager if prompt embedding service is not available
	if pm := prompts.NewPromptManager(); pm != nil {
		prompt, err := pm.GetPrompt(prompts.PromptTypeKeywordExtraction)
		if err == nil {
			userContent, err := pm.RenderUserPrompt(
				prompts.PromptTypeKeywordExtraction,
				map[string]string{"query": query},
			)
			if err == nil {
				messages := []pkgopenai.Message{
					{
						Role:    "system",
						Content: prompt.System,
					},
					{
						Role:    "user",
						Content: userContent,
					},
				}

				resp, err := s.LLM.CreateChatCompletionWithDefaults(s.Config.Services.LLM.Model, messages)

				if err != nil {
					logger.Get().Error("LLMå…³é”®è¯æå–å¤±è´¥", slog.Any("error", err))
					// é™çº§ä¸ºç®€å•åˆ†è¯
					return pkgutils.ExtractBasicKeywords(query), nil
				}

				if len(resp.Choices) == 0 {
					return pkgutils.ExtractBasicKeywords(query), nil
				}

				logger.Get().Info("å…³é”®è¯ LLM", slog.Any("resp", resp))
				// è§£æLLMè¿”å›çš„XMLæ ¼å¼å…³é”®è¯
				content := resp.Choices[0].Message.Content
				keywords := s.parseKeywordsXML(content)

				if len(keywords) == 0 {
					// å¦‚æœXMLè§£æå¤±è´¥ï¼Œå°è¯•æŒ‰è¡Œè§£æï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
					lines := strings.Split(strings.TrimSpace(content), "\n")
					for _, line := range lines {
						keyword := strings.TrimSpace(line)
						// è·³è¿‡XMLæ ‡ç­¾
						if strings.HasPrefix(keyword, "<") && strings.HasSuffix(keyword, ">") {
							continue
						}
						if keyword != "" && len(keyword) > 1 {
							keywords = append(keywords, keyword)
						}
					}
				}

				if len(keywords) == 0 {
					return pkgutils.ExtractBasicKeywords(query), nil
				}

				return keywords, nil
			}
		}
	}

	// Final fallback to basic keywords if all else fails
	return pkgutils.ExtractBasicKeywords(query), nil

}

// parseKeywordsXML parses XML-formatted keyword response from LLM.
//
// This function extracts keywords from XML tags in the format:
//
//	<keyword>word</keyword>
//
// It handles both well-formed XML and loosely formatted responses,
// providing robust parsing even when the LLM output is imperfect.
func (s *RagServer) parseKeywordsXML(xmlContent string) []string {
	var keywords []string

	// ç®€å•çš„XMLè§£æï¼Œæå–<keyword>æ ‡ç­¾å†…å®¹
	lines := strings.Split(xmlContent, "\n")
	for _, line := range lines {
		line = strings.TrimSpace(line)

		// æŸ¥æ‰¾ <keyword>xxx</keyword> æ¨¡å¼
		if strings.HasPrefix(line, "<keyword>") && strings.HasSuffix(line, "</keyword>") {
			// æå–æ ‡ç­¾ä¹‹é—´çš„å†…å®¹
			start := len("<keyword>")
			end := len(line) - len("</keyword>")
			if end > start {
				keyword := strings.TrimSpace(line[start:end])
				// éªŒè¯å…³é”®è¯æœ‰æ•ˆæ€§
				if keyword != "" && len(keyword) > 1 && !strings.Contains(keyword, "<") {
					keywords = append(keywords, keyword)
				}
			}
		}
	}

	// å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•æ›´å®½æ¾çš„è§£æ
	if len(keywords) == 0 {
		// ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾æ‰€æœ‰<keyword>å†…å®¹</keyword>æ¨¡å¼
		content := strings.ReplaceAll(xmlContent, "\n", " ")
		parts := strings.Split(content, "<keyword>")
		for _, part := range parts {
			if idx := strings.Index(part, "</keyword>"); idx > 0 {
				keyword := strings.TrimSpace(part[:idx])
				if keyword != "" && len(keyword) > 1 && !strings.Contains(keyword, "<") {
					keywords = append(keywords, keyword)
				}
			}
		}
	}

	return keywords
}

// rerankChunksWithKeywords performs intelligent reranking using a hybrid algorithm.
//
// This function delegates to search.RerankChunksWithKeywords which combines:
//   - Vector similarity (40% weight)
//   - Keyword matching (30% weight)
//   - Phrase matching (20% weight)
//   - Content quality (10% weight)
//
// Parameters are configured with maxChunks=5 and minSimilarity=0.25.
func (s *RagServer) rerankChunksWithKeywords(chunks []adapters.ChunkSearchResult, query string, keywords []string) []adapters.ChunkSearchResult {
	return search.RerankChunksWithKeywords(chunks, query, keywords, 5, 0.25)
}

// calculateAdvancedChunkScore calculates multi-dimensional scoring for a chunk.
//
// This function delegates to search.CalculateAdvancedScore for the actual
// scoring implementation.
func (s *RagServer) calculateAdvancedChunkScore(chunk adapters.ChunkSearchResult, query string, keywords []string) float64 {
	return search.CalculateAdvancedScore(chunk, query, keywords)
}

// generateContextSummary generates an intelligent context summary using LLM.
//
// Based on retrieved document chunks, this function uses the LLM to perform
// deep analysis and intelligent summarization, generating high-quality,
// structured responses tailored to the user's query.
//
// The function expects XML-formatted output from the LLM and falls back to
// generateBasicContextSummary if the LLM is unavailable or returns an error.
func (s *RagServer) generateContextSummary(ctx context.Context, chunks []adapters.ChunkSearchResult, query string) (string, error) {
	if len(chunks) == 0 {
		return "", fmt.Errorf("no chunks to summarize")
	}

	// Build raw context for LLM analysis
	rawContextBuilder := strings.Builder{}
	rawContextBuilder.WriteString("ä»¥ä¸‹æ˜¯ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š\n\n")

	for i, chunk := range chunks {
		cleanContent := s.cleanAndFormatChunkContent(chunk.Content)
		rawContextBuilder.WriteString(fmt.Sprintf("**ä¿¡æ¯ç‰‡æ®µ%d (ç›¸ä¼¼åº¦: %.3f):**\n", i+1, chunk.Similarity))
		rawContextBuilder.WriteString(cleanContent)

		if chunk.Metadata != nil {
			if chunkType, ok := chunk.Metadata["chunk_type"].(string); ok && chunkType != "" {
				rawContextBuilder.WriteString(fmt.Sprintf("\n*[ç±»å‹: %s]*", chunkType))
			}
		}
		rawContextBuilder.WriteString("\n\n")
	}

	var messages []openai.Message

	// Try to use prompt manager for context summary
	if s.promptEmbeddingService != nil {
		prompt, _, err := s.promptEmbeddingService.GetPromptWithEmbedding(prompts.PromptTypeContextSummary)
		if err == nil {
			userContent, err := s.promptEmbeddingService.GetPromptManager().RenderUserPrompt(
				prompts.PromptTypeContextSummary,
				map[string]string{
					"query":   query,
					"context": rawContextBuilder.String(),
				},
			)
			if err == nil {
				messages = []openai.Message{
					{
						Role:    "system",
						Content: prompt.System,
					},
					{
						Role:    "user",
						Content: userContent,
					},
				}
			}
		}
	}

	// Fallback to direct prompt manager if prompt service is not available
	if len(messages) == 0 {
		if pm := prompts.NewPromptManager(); pm != nil {
			prompt, err := pm.GetPrompt(prompts.PromptTypeContextSummary)
			if err == nil {
				userContent, err := pm.RenderUserPrompt(
					prompts.PromptTypeContextSummary,
					map[string]string{
						"query":   query,
						"context": rawContextBuilder.String(),
					},
				)
				if err == nil {
					messages = []openai.Message{
						{
							Role:    "system",
							Content: prompt.System,
						},
						{
							Role:    "user",
							Content: userContent,
						},
					}
				}
			}
		}
		// If prompt manager fails, return error
		if len(messages) == 0 {
			return s.generateBasicContextSummary(chunks, query), nil
		}
	}

	// è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½æ€»ç»“
	resp, err := s.LLM.CreateChatCompletionWithDefaults(s.Config.Services.LLM.Model, messages)
	if err != nil {
		logger.Get().Error("LLMæ™ºèƒ½æ€»ç»“å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ¨¡æ¿", slog.Any("error", err))
		// é™çº§åˆ°åŸºç¡€æ¨¡æ¿æ–¹æ¡ˆ
		return s.generateBasicContextSummary(chunks, query), nil
	}

	if len(resp.Choices) == 0 || resp.Choices[0].Message.Content == "" {
		logger.Get().Warn("LLMè¿”å›ç©ºå†…å®¹ï¼Œå›é€€åˆ°åŸºç¡€æ¨¡æ¿")
		return s.generateBasicContextSummary(chunks, query), nil
	}

	intelligentSummary := resp.Choices[0].Message.Content

	// æ·»åŠ ç³»ç»Ÿæ ‡è¯†å’Œä½¿ç”¨è¯´æ˜
	var finalSummary strings.Builder
	finalSummary.WriteString(intelligentSummary)
	finalSummary.WriteString("\n\n---\n\n")
	finalSummary.WriteString("æç¤º: ä»¥ä¸Šå›ç­”åŸºäºçŸ¥è¯†åº“æ£€ç´¢ç»“æœç”Ÿæˆï¼Œå¦‚éœ€äº†è§£æ›´è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥å°è¯•è°ƒæ•´æŸ¥è¯¢å…³é”®è¯æˆ–æå‡ºæ›´å…·ä½“çš„é—®é¢˜ã€‚")

	logger.Get().Info("LLMæ™ºèƒ½æ€»ç»“ç”ŸæˆæˆåŠŸ",
		slog.String("query", query),
		slog.Int("chunks_count", len(chunks)),
		slog.Int("summary_length", len(intelligentSummary)),
	)

	return finalSummary.String(), nil
}

// generateBasicContextSummary provides basic template-based summarization.
//
// This is a fallback solution when the LLM service is unavailable.
// It provides basic information organization and formatting using
// predefined templates and query type analysis.
func (s *RagServer) generateBasicContextSummary(chunks []adapters.ChunkSearchResult, query string) string {
	var contextBuilder strings.Builder

	contextBuilder.WriteString(fmt.Sprintf("## å…³äºã€Œ%sã€çš„ç›¸å…³ä¿¡æ¯\n\n", query))

	// æ™ºèƒ½åˆ†ææŸ¥è¯¢ç±»å‹ï¼Œæä¾›é’ˆå¯¹æ€§çš„å¼•å¯¼
	queryType := s.analyzeQueryType(query)
	contextBuilder.WriteString(s.generateQueryTypeGuidance(queryType))

	// æ·»åŠ æ£€ç´¢åˆ°çš„ä¿¡æ¯
	contextBuilder.WriteString("### ğŸ“š æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹\n\n")

	for i, chunk := range chunks {
		cleanContent := s.cleanAndFormatChunkContent(chunk.Content)

		contextBuilder.WriteString(fmt.Sprintf("**%d. ç›¸å…³ä¿¡æ¯** (ç›¸ä¼¼åº¦: %.2f)\n\n", i+1, chunk.Similarity))
		contextBuilder.WriteString(cleanContent)
		contextBuilder.WriteString("\n\n")

		// æ·»åŠ ç®€å•çš„å…ƒæ•°æ®
		if chunk.Metadata != nil {
			if chunkType, ok := chunk.Metadata["chunk_type"].(string); ok && chunkType != "" {
				contextBuilder.WriteString(fmt.Sprintf("*ä¿¡æ¯ç±»å‹: %s*\n\n", chunkType))
			}
		}
	}

	// æ·»åŠ æ™ºèƒ½æ€»ç»“
	contextBuilder.WriteString("### ğŸ’¡ ä¿¡æ¯æ€»ç»“\n\n")
	contextBuilder.WriteString("åŸºäºä»¥ä¸Šæ£€ç´¢ç»“æœï¼Œè¿™äº›ä¿¡æ¯æ¶µç›–äº†æ‚¨æŸ¥è¯¢çš„ç›¸å…³æ–¹é¢ã€‚")
	contextBuilder.WriteString(s.generateQuerySpecificSummary(query, chunks))
	contextBuilder.WriteString("\n\n")

	contextBuilder.WriteString("å¦‚éœ€äº†è§£æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œå»ºè®®æ‚¨ï¼š\n")
	contextBuilder.WriteString("- æŸ¥çœ‹ä¸Šè¿°å…·ä½“çš„ä¿¡æ¯ç‰‡æ®µ\n")
	contextBuilder.WriteString("- å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æŸ¥è¯¢\n")
	contextBuilder.WriteString("- æå‡ºæ›´è¯¦ç»†çš„é—®é¢˜ä»¥è·å¾—ç²¾å‡†ç­”æ¡ˆ\n")

	return contextBuilder.String()
}

// analyzeQueryType analyzes the semantic type of a query.
//
// This function delegates to utils.AnalyzeQueryType for the actual analysis.
func (s *RagServer) analyzeQueryType(query string) pkgutils.QueryType {
	return pkgutils.AnalyzeQueryType(query)
}

// generateQueryTypeGuidance generates guidance text based on query type.
//
// This function delegates to utils.GetQueryTypeGuidance for the actual
// guidance generation.
func (s *RagServer) generateQueryTypeGuidance(queryType pkgutils.QueryType) string {
	return pkgutils.GetQueryTypeGuidance(queryType)
}

// generateQuerySpecificSummary generates a query-specific summary.
//
// This function delegates to utils.GetQuerySpecificSummary for the actual
// summary generation based on the analyzed query type.
func (s *RagServer) generateQuerySpecificSummary(query string, _ []adapters.ChunkSearchResult) string {
	queryType := pkgutils.AnalyzeQueryType(query)
	return pkgutils.GetQuerySpecificSummary(queryType)
}

// generateSmartResponse generates template-based fallback responses.
//
// This function provides a fallback response generation mechanism when
// the LLM service is unavailable. It uses predefined templates and rules
// to generate structured responses, ensuring users always receive useful
// information even when advanced services are down.
// generateEmbedding generates embeddings for the given text using the configured embedding service.
//
// This function uses the embedding client to convert text into vector representations
// that can be used for semantic similarity searches. It handles the conversion from
// the embedding response format to the expected []float32 format.
//
// Returns an error if the embedding service is unavailable or if the embedding
// generation fails.

func (s *RagServer) generateSmartResponse(query string, chunks []adapters.ChunkSearchResult) string {
	var responseBuilder strings.Builder

	// æ·»åŠ é—®é¢˜ç†è§£
	responseBuilder.WriteString(fmt.Sprintf("å…³äºã€Œ%sã€ï¼Œæˆ‘åœ¨ç›¸å…³æ–‡æ¡£ä¸­æ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n", query))

	// åˆ†æå’Œæ€»ç»“å†…å®¹
	for i, chunk := range chunks {
		content := s.cleanAndFormatChunkContent(chunk.Content)
		responseBuilder.WriteString(fmt.Sprintf("**%d. ç›¸å…³ä¿¡æ¯ (ç›¸ä¼¼åº¦: %.2f)**\n", i+1, chunk.Similarity))
		responseBuilder.WriteString(content)
		responseBuilder.WriteString("\n\n")
	}

	// æ·»åŠ æ™ºèƒ½æ€»ç»“
	responseBuilder.WriteString("**æ€»ç»“ï¼š**\n")
	responseBuilder.WriteString("åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹ï¼Œè¿™äº›ä¿¡æ¯æ¶µç›–äº†æ‚¨è¯¢é—®çš„ä¸»è¦æ–¹é¢ã€‚")

	// æ ¹æ®æŸ¥è¯¢ç±»å‹æ·»åŠ ä¸åŒçš„å»ºè®®
	queryLower := strings.ToLower(query)
	if strings.Contains(queryLower, "ç»éªŒ") || strings.Contains(queryLower, "å·¥ä½œ") {
		responseBuilder.WriteString("ä»å·¥ä½œç»éªŒçš„è§’åº¦æ¥çœ‹ï¼Œæ–‡æ¡£ä¸­æåˆ°çš„ç›¸å…³èƒŒæ™¯å’Œå®è·µç»éªŒå¯ä»¥ä¸ºæ‚¨æä¾›å‚è€ƒã€‚")
	} else if strings.Contains(queryLower, "æŠ€æœ¯") || strings.Contains(queryLower, "å¼€å‘") {
		responseBuilder.WriteString("ä»æŠ€æœ¯è§’åº¦åˆ†æï¼Œç›¸å…³çš„æŠ€æœ¯æ ˆå’Œå¼€å‘ç»éªŒåœ¨æ–‡æ¡£ä¸­æœ‰è¯¦ç»†æè¿°ã€‚")
	} else if strings.Contains(queryLower, "é¡¹ç›®") {
		responseBuilder.WriteString("é¡¹ç›®ç›¸å…³çš„ä¿¡æ¯æ˜¾ç¤ºäº†å…·ä½“çš„å®æ–½ç»éªŒå’Œæˆæœã€‚")
	}

	responseBuilder.WriteString("\n\nğŸ’¡ å¦‚éœ€äº†è§£æ›´å…·ä½“çš„ä¿¡æ¯ï¼Œå»ºè®®æ‚¨æŸ¥çœ‹ä¸Šè¿°ç›¸å…³å†…å®¹æˆ–æå‡ºæ›´è¯¦ç»†çš„é—®é¢˜ã€‚")

	return responseBuilder.String()
}
